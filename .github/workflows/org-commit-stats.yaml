name: Org Commit Insights (30d summary + weekly LOC default on + 1500d chart)

on:
  workflow_dispatch:
    inputs:
      compute_loc:
        description: "Compute LOC (true/false). Default true."
        required: false
        default: "true"
  schedule:
    - cron: "0 6 * * 1" # every Monday 06:00 UTC

env:
  ORG_NAME: Ludo-MVP

permissions:
  contents: write

concurrency:
  group: org-commit-insights
  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Verify gh is available
        run: gh --version

      - name: Generate GitHub App token
        id: app-token
        uses: actions/create-github-app-token@v1
        with:
          app-id: ${{ secrets.GH_APP_ID }}
          private-key: ${{ secrets.GH_APP_PRIVATE_KEY }}
          owner: ${{ env.ORG_NAME }}

      - name: Install deps (matplotlib always, cloc when LOC enabled)
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install matplotlib

          sudo apt-get update -y
          sudo apt-get install -y jq

          COMPUTE_LOC="true"
          if [[ "${GITHUB_EVENT_NAME}" != "schedule" ]]; then
            COMPUTE_LOC="${{ inputs.compute_loc || 'true' }}"
          fi

          if [[ "${COMPUTE_LOC}" == "true" ]]; then
            sudo apt-get install -y cloc
          fi

      - name: Generate report (Markdown + optional LOC + 1500d chart)
        env:
          GH_TOKEN: ${{ steps.app-token.outputs.token }}
          ORG_LOGIN: ${{ env.ORG_NAME }}
          ORG_LABEL: "Ludo Labs AG"
          SELF_REPO: ${{ github.repository }}
          EVENT_NAME: ${{ github.event_name }}
          COMPUTE_LOC_INPUT: ${{ inputs.compute_loc || 'true' }}
        shell: bash
        run: |
          set -euo pipefail

          COMPUTE_LOC="true"
          if [[ "${EVENT_NAME}" != "schedule" ]]; then
            COMPUTE_LOC="${COMPUTE_LOC_INPUT}"
          fi
          export COMPUTE_LOC

          python - <<'PY'
          import os, json, subprocess
          from datetime import datetime, timedelta, timezone, date
          from collections import defaultdict

          ORG_LOGIN = os.environ["ORG_LOGIN"]
          ORG_LABEL = os.environ.get("ORG_LABEL", ORG_LOGIN)
          GH_TOKEN = os.environ["GH_TOKEN"]
          SELF_REPO = (os.environ.get("SELF_REPO") or "").lower()
          COMPUTE_LOC = (os.environ.get("COMPUTE_LOC") or "true").lower() == "true"

          OUT_MD = "commits.md"
          OUT_PNG = "commits_1500d.png"

          since_30d_dt = datetime.now(timezone.utc) - timedelta(days=30)
          since_30d_date = since_30d_dt.date().isoformat()
          since_30d_iso = f"{since_30d_date}T00:00:00Z"

          start_1500d_dt = datetime.now(timezone.utc) - timedelta(days=1500)
          since_1500d_iso = start_1500d_dt.isoformat().replace("+00:00", "Z")

          def sh(cmd, check=True):
            return subprocess.run(cmd, check=check, text=True, capture_output=True)

          def gh(cmd, check=True):
            r = sh(["gh"] + cmd, check=check)
            return r.stdout

          def gh_graphql(query: str, variables: dict):
            out = gh(["api", "graphql", "-f", f"query={query}", "-f", f"variables={json.dumps(variables)}"])
            return json.loads(out)

          def list_accessible_repos():
            # Critical fix: use --jq with --paginate to emit one repo per line (safe streaming output).
            # This avoids JSON "Extra data" errors from concatenated pages.
            out = gh([
              "api",
              "installation/repositories?per_page=100",
              "--paginate",
              "--jq",
              ".repositories[].full_name"
            ])
            repos = [line.strip() for line in out.splitlines() if line.strip()]

            filtered = []
            for full in repos:
              if "/" not in full:
                continue
              name = full.split("/", 1)[1]

              if SELF_REPO and full.lower() == SELF_REPO:
                continue
              if name.lower() == "stats" or full.lower().endswith("/stats"):
                continue
              if "backup" in name.lower() or "backup" in full.lower():
                continue

              filtered.append(full)

            # de-dup and stable order
            return sorted(set(filtered))

          Q_COMMITS_30D = r"""
          query($owner:String!, $name:String!, $since:GitTimestamp!) {
            repository(owner:$owner, name:$name) {
              defaultBranchRef {
                target {
                  ... on Commit {
                    history(since:$since) { totalCount }
                  }
                }
              }
            }
          }
          """

          Q_HISTORY_PAGE = r"""
          query($owner:String!, $name:String!, $since:GitTimestamp!, $after:String) {
            repository(owner:$owner, name:$name) {
              defaultBranchRef {
                target {
                  ... on Commit {
                    history(since:$since, first:100, after:$after) {
                      pageInfo { hasNextPage endCursor }
                      nodes { committedDate }
                    }
                  }
                }
              }
            }
          }
          """

          def commits_last_30d(repo_full: str) -> int:
            owner, name = repo_full.split("/", 1)
            data = gh_graphql(Q_COMMITS_30D, {"owner": owner, "name": name, "since": since_30d_iso})
            repo = (data.get("data") or {}).get("repository") or {}
            dbr = repo.get("defaultBranchRef")
            if not dbr:
              return 0
            target = dbr.get("target") or {}
            hist = target.get("history") or {}
            return int(hist.get("totalCount") or 0)

          def loc_repo(repo_full: str) -> int:
            tmp = f"__repo_tmp_{os.getpid()}_{abs(hash(repo_full))}"
            url = f"https://x-access-token:{GH_TOKEN}@github.com/{repo_full}.git"
            try:
              sh(["git", "clone", "--quiet", "--depth", "1", url, tmp], check=True)
            except subprocess.CalledProcessError:
              sh(["rm", "-rf", tmp], check=False)
              return 0

            try:
              r = sh(["cloc", tmp, "--json", "--quiet"], check=True)
              j = json.loads(r.stdout or "{}")
              return int(((j.get("SUM") or {}).get("code")) or 0)
            except Exception:
              return 0
            finally:
              sh(["rm", "-rf", tmp], check=False)

          def week_key(d: date) -> str:
            monday = d - timedelta(days=d.weekday())
            return monday.isoformat()

          def add_repo_1500d(repo_full: str, weekly: dict):
            owner, name = repo_full.split("/", 1)
            after = None
            while True:
              data = gh_graphql(Q_HISTORY_PAGE, {"owner": owner, "name": name, "since": since_1500d_iso, "after": after})
              repo = (data.get("data") or {}).get("repository") or {}
              dbr = repo.get("defaultBranchRef")
              if not dbr:
                return
              target = dbr.get("target") or {}
              hist = target.get("history") or {}
              nodes = hist.get("nodes") or []

              for n in nodes:
                cd = n.get("committedDate")
                if cd:
                  d = date.fromisoformat(cd[:10])
                  weekly[week_key(d)] += 1

              pi = hist.get("pageInfo") or {}
              if not pi.get("hasNextPage"):
                return
              after = pi.get("endCursor")

          def render_chart(weekly: dict):
            import matplotlib.pyplot as plt

            end_dt = datetime.now(timezone.utc)
            start_dt = end_dt - timedelta(days=1500)

            end_date = end_dt.date()
            start_date = start_dt.date()

            start_monday = start_date - timedelta(days=start_date.weekday())
            end_monday = end_date - timedelta(days=end_date.weekday())

            labels, values = [], []
            cur = start_monday
            while cur <= end_monday:
              k = cur.isoformat()
              labels.append(k)
              values.append(int(weekly.get(k, 0)))
              cur += timedelta(days=7)

            plt.figure(figsize=(12, 4))
            plt.plot(range(len(values)), values)

            step = max(1, len(labels) // 12)
            plt.xticks(
              range(0, len(labels), step),
              [labels[i] for i in range(0, len(labels), step)],
              rotation=45,
              ha="right"
            )
            plt.ylabel("Commits (weekly)")
            plt.title("Org commits trend (last 1500 days, weekly, active repos, default branches)")
            plt.tight_layout()
            plt.savefig(OUT_PNG, dpi=150)

          def main():
            repos = list_accessible_repos()

            processed = 0
            skipped = 0
            total_commits = 0
            active_repos = []

            for repo in repos:
              try:
                c = commits_last_30d(repo)
                processed += 1
                total_commits += c
                if c > 0:
                  active_repos.append(repo)
              except Exception:
                skipped += 1

            total_loc = 0
            loc_counted = 0
            loc_skipped = 0
            if COMPUTE_LOC and active_repos:
              for repo in active_repos:
                loc = loc_repo(repo)
                if loc > 0:
                  total_loc += loc
                  loc_counted += 1
                else:
                  loc_skipped += 1

            weekly = defaultdict(int)
            for repo in active_repos:
              try:
                add_repo_1500d(repo, weekly)
              except Exception:
                pass

            if not weekly:
              import matplotlib.pyplot as plt
              plt.figure(figsize=(12, 4))
              plt.plot([0], [0])
              plt.title("Org commits (last 1500 days) â€” no data")
              plt.tight_layout()
              plt.savefig(OUT_PNG, dpi=150)
            else:
              render_chart(weekly)

            now = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M UTC")
            with open(OUT_MD, "w", encoding="utf-8") as f:
              f.write("# Organization Commit Insights\n\n")
              f.write(f"**Org:** {ORG_LABEL}  \n")
              f.write(f"**Commits period:** last 30 days (since {since_30d_date}, all branches)  \n")
              f.write("**LOC:** computed weekly for repos with commits > 0  \n" if COMPUTE_LOC else "**LOC:** not computed  \n")
              f.write(f"**Generated:** {now}\n\n")

              f.write("## Summary\n\n")
              f.write(f"- **Total commits (30d):** {total_commits}\n")
              f.write(f"- **Repositories counted (commits):** {processed}\n")
              f.write(f"- **Repositories skipped (commits):** {skipped}\n")
              f.write(f"- **Active repos (commits > 0):** {len(active_repos)}\n")
              f.write(f"- **Total LOC (active repos only):** {total_loc}\n")
              f.write(f"- **Repositories counted (LOC):** {loc_counted}\n")
              f.write(f"- **Repositories skipped (LOC):** {loc_skipped}\n\n")
              f.write(f"![Org commits trend (last 1500 days)]({OUT_PNG})\n")

          if __name__ == "__main__":
            main()
          PY

      - name: Commit & push report
        shell: bash
        run: |
          set -euo pipefail
          git config user.name "github-actions"
          git config user.email "actions@github.com"

          git add commits.md commits_1500d.png || true

          if git diff --cached --quiet; then
            echo "No changes to commit."
            exit 0
          fi

          git commit -m "Update org commit insights (30d summary + LOC + 1500d chart)"
          git push
